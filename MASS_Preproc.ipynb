{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c0d511-ebe6-4366-93bd-cdb67a68fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from mne.preprocessing import ICA\n",
    "from mne_icalabel import label_components\n",
    "from autoreject import AutoReject\n",
    "from scipy.signal.windows import hamming\n",
    "from scipy.stats import spearmanr\n",
    "from autoreject import get_rejection_threshold\n",
    "import gc\n",
    "from specparam import SpectralGroupModel\n",
    "#from specparam.analysis import get_band_peak_group\n",
    "from mne.preprocessing import create_eog_epochs, create_ecg_epochs\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib qt\n",
    "#the following line allows interactive plotting -> https://mne.discourse.group/t/semi-interactive-plot-with-matplotlib-backend/5094/2\n",
    "#matplotlib.use('Qt5Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3479ed7-9f0d-4ffd-853d-46268729e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/elizabethkaplan/Desktop/SS2_Data/01-02-0016-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 7255039  =      0.000 ... 28339.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/21s_2dfj2f195fl1lglp7jrw0000gn/T/ipykernel_26180/956330687.py:46: RuntimeWarning: The unit for channel(s) Resp Nasal has changed from V to NA.\n",
      "  raw.set_channel_types({k: v for k, v in ch_types.items() if k in raw.ch_names})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file.\n",
      "Saved N2 KC + spindle summary to: /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016_N2_event_summary.csv\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.1 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 8449 samples (33.004 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 59 - 61 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 59.35\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 59.10 Hz)\n",
      "- Upper passband edge: 60.65 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 60.90 Hz)\n",
      "- Filter length: 1691 samples (6.605 s)\n",
      "\n",
      "Not setting metadata\n",
      "348 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 348 events and 7681 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Estimating rejection dictionary for eeg\n",
      "0 bad epochs dropped\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 845 samples (3.301 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 287 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done 647 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=1)]: Done 881 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done 1151 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done 1457 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=1)]: Done 2177 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=1)]: Done 2591 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=1)]: Done 3041 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=1)]: Done 3527 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=1)]: Done 4607 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=1)]: Done 5201 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=1)]: Done 5831 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=1)]: Done 6497 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 20 channels (please be patient, this may take a while)\n",
      "Selecting by explained variance: 10 components\n",
      "Computing Extended Infomax ICA\n"
     ]
    }
   ],
   "source": [
    "### 1. SETUP \n",
    "#load in data and annotations \n",
    "subj_id = \"0016\" \n",
    "psg_path = \"/Users/elizabethkaplan/Desktop/SS2_Data/01-02-0016-PSG.edf\" \n",
    "annotation_file_path = \"/Users/elizabethkaplan/Desktop/SS2_Data/01-02-0016 KComplexes_E1.edf\" \n",
    "spindles_annotations = \"/Users/elizabethkaplan/Desktop/SS2_Data/01-02-0016 Spindles_E1.edf\" \n",
    "staging_file_path = '/Users/elizabethkaplan/Desktop/SS2_Data/01-02-0016-Base.edf' \n",
    "\n",
    "# preload=True loads the data into memory, enabling faster operations\n",
    "\n",
    "raw = mne.io.read_raw_edf(psg_path, preload=True) \n",
    "annot = mne.read_annotations(annotation_file_path) \n",
    "spindles = mne.read_annotations(spindles_annotations) \n",
    "stages = mne.read_annotations(staging_file_path)\n",
    "\n",
    "# Create output folders \n",
    "full_sess_name = f\"01-02-{subj_id}\" \n",
    "out_dir = Path(\"/Users/elizabethkaplan/Desktop/SS2_Results\") \n",
    "session_name = full_sess_name \n",
    "subject_model_dir = out_dir / full_sess_name / \"spectral_models\" \n",
    "subject_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Rename channels properly (run once) ---- \n",
    "rename_map = {} \n",
    "for ch in raw.ch_names: \n",
    "    if ch.startswith(\"EEG \"): \n",
    "        base = ch.replace(\"EEG \", \"\").replace(\"-CLE\", \"\") \n",
    "        rename_map[ch] = base \n",
    "        \n",
    "raw.rename_channels(rename_map, allow_duplicates=True)\n",
    "\n",
    "# ---- Attach montage AFTER rename ---- \n",
    "montage = mne.channels.make_standard_montage(\"standard_1020\") \n",
    "raw.set_montage(montage, match_case=False, on_missing=\"ignore\") \n",
    "\n",
    "# set ch types \n",
    "ch_types = { \"EOG Upper Vertic\": \"eog\", \n",
    "             \"EOG Lower Vertic\": \"eog\", \n",
    "             \"EOG Left Horiz\": \"eog\", \n",
    "             \"EOG Right Horiz\": \"eog\", \n",
    "             \"EMG Chin\": \"emg\", \"ECG ECGI\": \n",
    "             \"ecg\", \"Resp Nasal\": \"misc\", # or 'resp' if you prefer \n",
    "             # A2 is a mastoid ref; you can treat as EEG or misc depending on how you use it \n",
    "             \"A2\": \"eeg\",} # this is the renamed \"EEG A2-CLE\" }\n",
    "\n",
    "raw.set_channel_types({k: v for k, v in ch_types.items() if k in raw.ch_names})\n",
    "\n",
    "# attach montage \n",
    "montage = mne.channels.make_standard_montage(\"standard_1020\") \n",
    "raw.set_montage(montage, match_case=False, on_missing=\"ignore\")\n",
    "\n",
    "\n",
    "# N2 intervals from staging \n",
    "n2_intervals = [] \n",
    "for a in stages: \n",
    "    if a[\"description\"] == \"Sleep stage 2\": \n",
    "        tmin = float(a[\"onset\"]) \n",
    "        tmax = float(a[\"onset\"] + a[\"duration\"]) \n",
    "        n2_intervals.append((tmin, tmax)) \n",
    "    \n",
    "if not n2_intervals: \n",
    "    raise RuntimeError(\"No 'Sleep stage 2' intervals found.\")\n",
    "\n",
    "#merge intervals if they are close in time \n",
    "def merge_intervals(intervals, gap=0.0): \n",
    "    \"\"\"Merge intervals that overlap or are within gap seconds.\"\"\" \n",
    "    if not intervals: \n",
    "        return [] \n",
    "    intervals = sorted(intervals, key=lambda x: x[0]) \n",
    "    merged = [list(intervals[0])] \n",
    "    for start, end in intervals[1:]: \n",
    "        if start <= merged[-1][1] + gap: \n",
    "            merged[-1][1] = max(merged[-1][1], end) \n",
    "        else: merged.append([start, end]) \n",
    "    return [(s, e) for s, e in merged]\n",
    "\n",
    "n2_intervals_merged = merge_intervals(n2_intervals, gap=0.5) # 0.5s gap tolerance is safe \n",
    "\n",
    "# Build N2-only raw \n",
    "raws = [] \n",
    "for tmin, tmax in n2_intervals_merged: \n",
    "    r = raw.copy() \n",
    "    r.crop(tmin=tmin, tmax=tmax) \n",
    "    raws.append(r) \n",
    "    \n",
    "raw_n2 = mne.concatenate_raws(raws)\n",
    "\n",
    "# Map global time -> compressed N2 time \n",
    "def map_to_n2_time(t, intervals): \n",
    "    elapsed = 0.0 \n",
    "    for tmin, tmax in intervals: \n",
    "        if t < tmin: \n",
    "            break \n",
    "        if tmin <= t <= tmax: \n",
    "            return elapsed + (t - tmin) \n",
    "            elapsed += (tmax - tmin) \n",
    "    return None \n",
    "\n",
    "# Remap KC + spindle annotations \n",
    "all_annot = annot + spindles \n",
    "\n",
    "new_onsets, new_durations, new_desc = [], [], [] \n",
    "for a in all_annot: \n",
    "    t_new = map_to_n2_time(float(a[\"onset\"]), n2_intervals_merged) \n",
    "    if t_new is not None: \n",
    "        new_onsets.append(t_new) \n",
    "        new_durations.append(float(a[\"duration\"])) \n",
    "        new_desc.append(a[\"description\"])\n",
    "\n",
    "annot_on_n2_timeline = mne.Annotations(new_onsets, new_durations, new_desc) \n",
    "raw_n2.set_annotations(annot_on_n2_timeline) \n",
    "\n",
    "# Save annotations \n",
    "annot_on_n2_timeline.save( os.path.join(out_dir, f\"{session_name}_N2_annotations.csv\"), overwrite=True )\n",
    "\n",
    "## Save info on sleep durations \n",
    "total_sec = raw.times[-1] \n",
    "n2_sec = raw_n2.times[-1] \n",
    "\n",
    "dur_df = pd.DataFrame([{ \"session\": full_sess_name, \n",
    "                         \"total_duration_sec\": total_sec, \n",
    "                         \"total_duration_hr\": total_sec / 3600, \n",
    "                         \"n2_duration_sec\": n2_sec, \n",
    "                         \"n2_duration_hr\": n2_sec / 3600, }]) \n",
    "\n",
    "out_path = subject_model_dir / f\"{full_sess_name}_recording_durations.csv\" \n",
    "dur_df.to_csv(out_path, index=False)\n",
    "\n",
    "# Save info on KC and Spindle density \n",
    "# N2 duration \n",
    "n2_duration_sec = raw_n2.times[-1] \n",
    "n2_duration_min = n2_duration_sec / 60 \n",
    "\n",
    "# Count KCs in N2 \n",
    "kc_n2 = 0 \n",
    "for a in annot: \n",
    "    if map_to_n2_time(float(a[\"onset\"]), n2_intervals_merged) is not None: \n",
    "        kc_n2 += 1 \n",
    "        \n",
    "# Count spindles in N2 \n",
    "spindle_n2 = 0 \n",
    "for a in spindles: \n",
    "    if map_to_n2_time(float(a[\"onset\"]), n2_intervals_merged) is not None: \n",
    "        spindle_n2 += 1\n",
    "\n",
    "# per min \n",
    "kc_density_per_min = kc_n2 / n2_duration_min if n2_duration_min > 0 else float(\"nan\") \n",
    "spindle_density_per_min = spindle_n2 / n2_duration_min if n2_duration_min > 0 else float(\"nan\") \n",
    "\n",
    "# Save \n",
    "out_dir = \"/Users/elizabethkaplan/Desktop/SS2_Results\" \n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Use your subject/session variable if available \n",
    "session_name = full_sess_name # e.g., \"01-02-0002\" \n",
    "out_path = os.path.join(out_dir, f\"{session_name}_N2_event_summary.csv\") \n",
    "\n",
    "df = pd.DataFrame([{ \"session\": session_name, \n",
    "                     \"n2_duration_sec\": n2_duration_sec, \n",
    "                     \"n2_duration_min\": n2_duration_min, \n",
    "                     \"kc_count_n2\": kc_n2, \n",
    "                     \"kc_density_per_min_n2\": kc_density_per_min, \n",
    "                     \"spindle_count_n2\": spindle_n2, \n",
    "                     \"spindle_density_per_min_n2\": spindle_density_per_min, }])\n",
    "\n",
    "df.to_csv(out_path, index=False) \n",
    "print(\"Saved N2 KC + spindle summary to:\", out_path) \n",
    "\n",
    "## 2. PREPROCESSING \n",
    "\n",
    "#High and low pass filter \n",
    "raw_n2.filter(l_freq=0.1, h_freq=100.0) \n",
    "\n",
    "#notch filter \n",
    "raw_n2.notch_filter(freqs=60, picks=None, filter_length='auto', phase='zero') \n",
    "\n",
    "# Calculate artifact rejection threshold\n",
    "\n",
    "# 30 sec epochs \n",
    "events_n2_30 = mne.make_fixed_length_events(raw_n2, start=0, stop=raw_n2.times[-1], duration=30.0) \n",
    "epochs_n2_30 = mne.Epochs( raw_n2, \n",
    "                           events=events_n2_30, \n",
    "                           tmin=0.0, tmax=30.0, # 30 seconds \n",
    "                           baseline=None, \n",
    "                           picks=\"eeg\", \n",
    "                           preload=True, \n",
    "                           reject_by_annotation=True )\n",
    "\n",
    "# calc amplitude thrshold \n",
    "reject = get_rejection_threshold(epochs_n2_30, decim=1)\n",
    "\n",
    "# remove signal that exceeds threshold \n",
    "epochs_n2_30_clean = epochs_n2_30.copy().drop_bad(reject=reject) \n",
    "\n",
    "# --- ICA prep --- \n",
    "epochs_for_ica = epochs_n2_30_clean.copy().filter(l_freq=1.0, h_freq=40.0) \n",
    "ica = ICA( n_components=0.99, # adapts to channel count \n",
    "           max_iter=\"auto\", \n",
    "           method=\"infomax\", \n",
    "           random_state=97, \n",
    "           fit_params=dict(extended=True), ) \n",
    "ica.fit(epochs_for_ica)\n",
    "\n",
    "# ICLabel \n",
    "ic_labels = label_components(epochs_for_ica, ica, method=\"iclabel\") \n",
    "labels = ic_labels[\"labels\"] # Keep \"brain\" (and optionally \"other\"); exclude the rest \n",
    "exclude_idx = [i for i, lab in enumerate(labels) if lab not in (\"brain\", \"other\")] \n",
    "ica.exclude = exclude_idx \n",
    "\n",
    "print(\"ICLabel counts:\", {lab: sum(l == lab for l in labels) for lab in set(labels)}) \n",
    "print(f\"Excluding {len(ica.exclude)} components: {ica.exclude}\")\n",
    "\n",
    "# Optional: inspect what you're excluding (only if there are any) \n",
    "if len(ica.exclude) > 0: \n",
    "    ica.plot_components(picks=ica.exclude[:20]) \n",
    "else: print(\"No ICs marked for exclusion; skipping ica.plot_components().\") \n",
    "\n",
    "# Apply ICA \n",
    "epochs_n2_30_ica = epochs_n2_30_clean.copy() \n",
    "epochs_n2_30_ica = epochs_n2_30_ica.pick_types(eeg=True) \n",
    "epochs_n2_30_ica = epochs_n2_30_ica.set_eeg_reference(\"average\", projection=False) # Safe to call even if ica.exclude == [] \n",
    "ica.apply(epochs_n2_30_ica) \n",
    "print(\"Done. Final cleaned epochs:\", epochs_n2_30_ica)\n",
    "\n",
    "### Rereference to mastoid \n",
    "epochs_n2_30_ica.set_eeg_reference(ref_channels=[\"A2\"], \n",
    "                                   projection=False) \n",
    "epochs_n2_30_ica.drop_channels([\"A2\"]) \n",
    "\n",
    "### Autorejection \n",
    "N_JOBS = 10 \n",
    "ar = AutoReject(n_jobs=N_JOBS, random_state=42, verbose=False, picks=\"eeg\") \n",
    "epochs_ar_final, reject_log = ar.fit_transform(epochs_n2_30_ica, return_log=True) \n",
    "\n",
    "print(f\"AutoReject removed {len(epochs_n2_30_ica) - len(epochs_ar_final)}/{len(epochs_n2_30_ica)} epochs\") \n",
    "print(\"Number of epochs after autoreject:\", len(epochs_ar_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169f6346-8bf2-44e8-926b-b27447a45cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved epoch time mapping to: /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_epoch_time_map.csv\n",
      "Dropped 1759 annotations (fell in rejected time)\n",
      "Dropped 0 annotations (duration crossed a rejected/boundary region)\n",
      "Writing /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_raw.fif\n",
      "Closing /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_raw.fif\n",
      "[done]\n",
      "Saved cleaned raw: /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_raw.fif\n",
      "Saved cleaned annotations: /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_annotations.csv\n",
      "Saved cleaned annotations table: /Users/elizabethkaplan/Desktop/SS2_Results/01-02-0016/cleaned/01-02-0016_N2_clean_autoreject_annotations_table.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SAVE CLEANED DATA + UPDATED ANNOTATIONS (post-AutoReject)\n",
    "# Robust version: uses epochs_ar_final.events (no .selection)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- output folder ----\n",
    "save_root = Path(\"/Users/elizabethkaplan/Desktop/SS2_Results\") / full_sess_name / \"cleaned\"\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_fif_path   = save_root / f\"{full_sess_name}_N2_clean_autoreject_raw.fif\"\n",
    "ann_mne_path   = save_root / f\"{full_sess_name}_N2_clean_autoreject_annotations.csv\"\n",
    "ann_table_path = save_root / f\"{full_sess_name}_N2_clean_autoreject_annotations_table.csv\"\n",
    "epoch_map_path = save_root / f\"{full_sess_name}_N2_clean_autoreject_epoch_time_map.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# 1) Build gapless cleaned Raw from kept 30s epochs\n",
    "# -----------------------\n",
    "data = epochs_ar_final.get_data()  # (n_kept_epochs, n_ch, n_samp)\n",
    "n_ep, n_ch, n_samp = data.shape\n",
    "data_cont = data.transpose(1, 0, 2).reshape(n_ch, n_ep * n_samp)\n",
    "\n",
    "raw_clean = mne.io.RawArray(data_cont, epochs_ar_final.info.copy(), verbose=False)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Old->New time mapping using kept epochs' original start times\n",
    "# -----------------------\n",
    "sfreq = raw_n2.info[\"sfreq\"]\n",
    "epoch_len = epochs_ar_final.tmax - epochs_ar_final.tmin  # should be 30.0\n",
    "\n",
    "# Old start times (raw_n2 timeline) of the KEPT epochs:\n",
    "# epochs_ar_final.events[:, 0] are sample indices relative to raw_n2 start\n",
    "starts_old = epochs_ar_final.events[:, 0] / sfreq\n",
    "\n",
    "# New start times are back-to-back (gapless)\n",
    "starts_new = np.arange(len(starts_old)) * epoch_len\n",
    "\n",
    "# Save mapping table\n",
    "pd.DataFrame({\n",
    "    \"kept_epoch_order\": np.arange(len(starts_old)),\n",
    "    \"old_start_s\": starts_old,\n",
    "    \"new_start_s\": starts_new,\n",
    "    \"epoch_len_s\": epoch_len,\n",
    "}).to_csv(epoch_map_path, index=False)\n",
    "print(\"Saved epoch time mapping to:\", epoch_map_path)\n",
    "\n",
    "# Helper: map old time -> new time if it falls inside a kept epoch\n",
    "# Use searchsorted for speed and correctness even if old epochs are not perfectly regular.\n",
    "starts_old_sorted_idx = np.argsort(starts_old)\n",
    "starts_old_sorted = starts_old[starts_old_sorted_idx]\n",
    "starts_new_sorted = starts_new[starts_old_sorted_idx]\n",
    "\n",
    "def map_old_to_new_time(t_old: float):\n",
    "    j = np.searchsorted(starts_old_sorted, t_old, side=\"right\") - 1\n",
    "    if j < 0:\n",
    "        return None\n",
    "    s_old = float(starts_old_sorted[j])\n",
    "    if t_old >= s_old + epoch_len:\n",
    "        return None\n",
    "    s_new = float(starts_new_sorted[j])\n",
    "    return s_new + (t_old - s_old)\n",
    "\n",
    "# -----------------------\n",
    "# 3) Remap annotations to the cleaned timeline\n",
    "# STRICT: keep only events fully contained within a kept epoch\n",
    "# -----------------------\n",
    "old_ann = annot_on_n2_timeline  # annotations on raw_n2 timeline\n",
    "\n",
    "new_onsets, new_durs, new_desc = [], [], []\n",
    "dropped_outside = 0\n",
    "dropped_boundary = 0\n",
    "\n",
    "for onset, dur, desc in zip(old_ann.onset, old_ann.duration, old_ann.description):\n",
    "    onset = float(onset)\n",
    "    dur = float(dur)\n",
    "\n",
    "    onset_new = map_old_to_new_time(onset)\n",
    "    if onset_new is None:\n",
    "        dropped_outside += 1\n",
    "        continue\n",
    "\n",
    "    # Ensure full duration stays inside the same kept epoch\n",
    "    end_new = map_old_to_new_time(onset + dur)\n",
    "    if end_new is None:\n",
    "        dropped_boundary += 1\n",
    "        continue\n",
    "\n",
    "    new_onsets.append(onset_new)\n",
    "    new_durs.append(dur)\n",
    "    new_desc.append(str(desc))\n",
    "\n",
    "print(f\"Dropped {dropped_outside} annotations (fell in rejected time)\")\n",
    "print(f\"Dropped {dropped_boundary} annotations (duration crossed a rejected/boundary region)\")\n",
    "\n",
    "ann_clean = mne.Annotations(new_onsets, new_durs, new_desc)\n",
    "raw_clean.set_annotations(ann_clean)\n",
    "\n",
    "# Save annotations (MNE) + readable table\n",
    "ann_clean.save(ann_mne_path, overwrite=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"onset_s\": ann_clean.onset,\n",
    "    \"duration_s\": ann_clean.duration,\n",
    "    \"description\": ann_clean.description,\n",
    "}).to_csv(ann_table_path, index=False)\n",
    "\n",
    "# -----------------------\n",
    "# 4) Save cleaned raw (FIF)\n",
    "# -----------------------\n",
    "raw_clean.save(raw_fif_path, overwrite=True)\n",
    "\n",
    "print(\"Saved cleaned raw:\", raw_fif_path)\n",
    "print(\"Saved cleaned annotations:\", ann_mne_path)\n",
    "print(\"Saved cleaned annotations table:\", ann_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015637-69ce-4a0e-b6ea-026804363e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
